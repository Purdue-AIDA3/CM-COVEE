{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jshreeku/.conda/envs/cent7/2024.02-py311/js_video_models/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## general imports\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "## numpy\n",
    "import numpy as np\n",
    "\n",
    "## pandas\n",
    "import pandas as pd\n",
    "\n",
    "## scipy\n",
    "from scipy.io import loadmat\n",
    "\n",
    "## matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    if len(np.shape(data)) == 4:\n",
    "        data = np.transpose(np.stack(data), (0, 2, 1, 3))\n",
    "        original_shape = np.shape(data)\n",
    "        data = data.reshape(np.shape(data)[0], np.shape(data)[1], np.shape(data)[2] * np.shape(data)[3])\n",
    "\n",
    "        min_val = np.min(np.min(data, 2), 0)\n",
    "        max_val = np.max(np.max(data, 2), 0)\n",
    "        mean_val = np.mean(np.mean(data,2),0)\n",
    "        std_val = np.std(np.std(data,2),0)\n",
    "\n",
    "        for i in range(0, np.shape(data)[0]):\n",
    "            for j in range(0, np.shape(data)[1]):\n",
    "                data[i, j] = (data[i, j] - mean_val[j])/std_val[j]\n",
    "\n",
    "        data = data.reshape(original_shape)\n",
    "        data = np.transpose(data, (0, 2, 1, 3))\n",
    "        #\n",
    "\n",
    "    else:\n",
    "        data = np.stack(data)\n",
    "        temp = data.flatten()\n",
    "        temp = temp[temp>0]\n",
    "        min_val = np.min(temp, 0)\n",
    "        max_val = np.max(temp, 0)\n",
    "        mean_val = np.mean(temp, 0)\n",
    "        std_val = np.std(temp, 0)\n",
    "\n",
    "        if len(np.shape(data)) == 2:\n",
    "            for i in range(0, np.shape(data)[0]):\n",
    "                if (max_val - min_val) == 0:\n",
    "                    print(\"!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "                data[i] = ((data[i] - mean_val) / std_val)\n",
    "            data = np.stack(data)\n",
    "            data = data.reshape(np.shape(data)[0], 10, 30, 1)\n",
    "        elif len(np.shape(data)) == 3:\n",
    "            for i in range(0, np.shape(data)[0]):\n",
    "                for j in range(0, np.shape(data)[1]):\n",
    "                    data[i, j] = (data[i, j] - mean_val) / std_val\n",
    "            data = np.stack(data)\n",
    "            data = data.reshape(np.shape(data)[0], 10, 30, int(np.shape(data)[2]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_pad_image(image, target_size):\n",
    "    \"\"\"\n",
    "    Pads an image to a specified size.\n",
    "\n",
    "    Args:\n",
    "        image: The image to be padded (numpy array).\n",
    "        target_size: The desired size of the padded image (tuple of integers (height, width)).\n",
    "\n",
    "    Returns:\n",
    "        The padded image (numpy array).\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the original image shape\n",
    "    h, w = image.shape[:2]\n",
    "    old_image_height, old_image_width, channels = image.shape\n",
    "\n",
    "    # Calculate the amount of padding needed for each dimension\n",
    "    new_image_width = target_size[0]\n",
    "    new_image_height = target_size[1]\n",
    "    color = (0,0,0)\n",
    "    result = np.full((new_image_height,new_image_width, channels), color, dtype=np.uint8)\n",
    "\n",
    "    # compute center offset\n",
    "    x_center = (new_image_width - old_image_width) // 2\n",
    "    y_center = (new_image_height - old_image_height) // 2\n",
    "\n",
    "    # copy img image into center of result image\n",
    "    result[y_center:y_center+old_image_height,\n",
    "        x_center:x_center+old_image_width] = image\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_given_size(a, size):\n",
    "    ret = np.split(a, np.arange(size,len(a),size))\n",
    "    if ret[-1].shape[0] != size:\n",
    "        ret = ret[:-1]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SITUATION AWARENESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./situation_awareness_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_psd = []\n",
    "meta_pupil_diam = []\n",
    "meta_gaze_speed = []\n",
    "meta_fixation = []\n",
    "meta_scores = []\n",
    "meta_eye_landmarks = []\n",
    "meta_face_landmarks = []\n",
    "chunk_lengths = {}\n",
    "for user in sorted(os.listdir(\"../../../user_data/\")):\n",
    "    for task_name in sorted(os.listdir(\"../../../user_data/\" + user + \"/Task 1/\")):\n",
    "        if task_name == \".DS_Store\" or task_name == \"EEG\" or task_name == \"Eye Tracker\" or task_name == \"NASA\" or task_name == \"Video\":\n",
    "            continue\n",
    "        to_save = task_name if task_name[-1] != \"y\" else task_name[:-10]\n",
    "\n",
    "        # path to save dataset\n",
    "        save_path = \"../../../multimodal_dataset_SA/\" + user + \"_\" + to_save\n",
    "\n",
    "        # LOADING THE DATA\n",
    "        file = f\"./{user}/{task_name}.json\"\n",
    "        c = df[\"name\"] == file\n",
    "        filtered_df = df[c]\n",
    "        if len(filtered_df) == 0:\n",
    "            print(f\"User {user} on task {task_name} is not available in the csv file\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Parsing data files of user {user}, task {task_name}\")   \n",
    "\n",
    "            # PSD\n",
    "            psd = []\n",
    "            if task_name[-1] != 'y':\n",
    "                psd = loadmat(f\"../../../subject_data_for_chuhao/{user}/EEG/min_processed/backup/Task 1 Without Secondary Task/{to_save}.mat\")\n",
    "            else:\n",
    "                psd = loadmat(f\"../../../subject_data_for_chuhao/{user}/EEG/min_processed/backup/Task 1 With Secondary Task/{to_save}.mat\")       \n",
    "            meta_psd.append(psd['all_bands'][:,0:31,0:40])\n",
    "\n",
    "            # eye tracker\n",
    "            meta_fixation.append(np.loadtxt(f\"../../../subject_data_for_chuhao/{user}/Eye Tracker/Task 1/fixation/{task_name}.txt\"))\n",
    "            meta_pupil_diam.append(np.loadtxt(f\"../../../subject_data_for_chuhao/{user}/Eye Tracker/Task 1/pupil_diam/{task_name}.txt\"))\n",
    "            meta_gaze_speed.append(np.loadtxt(f\"../../../subject_data_for_chuhao/{user}/Eye Tracker/Task 1/gaze_speed/{task_name}.txt\"))\n",
    "\n",
    "            # facial landmarks\n",
    "            meta_eye_landmarks.append(np.loadtxt(f\"../../../subject_data_for_chuhao/{user}/Video/Task 1/Eye Landmarks/{task_name}.txt\",delimiter=','))\n",
    "            meta_face_landmarks.append(np.loadtxt(f\"../../../subject_data_for_chuhao/{user}/Video/Task 1/Face Landmarks/{task_name}.txt\",delimiter=','))\n",
    "            \n",
    "            #label\n",
    "            label = filtered_df['labels'].iloc[0]\n",
    "\n",
    "            # video\n",
    "            task_video_data = []\n",
    "            for frame in sorted(os.listdir(\"../../../user_data/\" + user + \"/Task 1/\" + task_name + \"/Images/\")):\n",
    "                image_path = \"../../../user_data/\" + user + \"/Task 1/\" + task_name + \"/Images/\" + frame\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                # print(image_path)\n",
    "                # image = custom_pad_image(image, target_size=(224, 224))         # to pad images from shape 112 x 112 to 224 x 224\n",
    "                numpy_data = np.asarray(image).astype(np.float32)\n",
    "                numpy_data = numpy_data.transpose(2, 1, 0)\n",
    "                numpy_data = numpy_data / 255\n",
    "                task_video_data.append(numpy_data)\n",
    "            task_video_data = np.array(task_video_data)\n",
    "\n",
    "            chunks = split_given_size(a=task_video_data, size=30)\n",
    "            chunk_lengths[f\"{user}/{to_save}\"] = len(chunks)\n",
    "\n",
    "# PREPROCESSING AND NORMALIZATION     \n",
    "eeg_normalized = normalize_data(meta_psd)       \n",
    "eye_normalized = np.concatenate((normalize_data(meta_pupil_diam), normalize_data(meta_gaze_speed)), axis=3)\n",
    "face_normalized = np.concatenate((normalize_data(meta_eye_landmarks), normalize_data(meta_face_landmarks)), axis=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eeg_normalized.shape)\n",
    "print(eye_normalized.shape)\n",
    "print(face_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "for user in sorted(os.listdir(\"../../../user_data/\")):\n",
    "    for task_name in sorted(os.listdir(\"../../../user_data/\" + user + \"/Task 1/\")):\n",
    "        if task_name == \".DS_Store\" or task_name == \"EEG\" or task_name == \"Eye Tracker\" or task_name == \"NASA\" or task_name == \"Video\":\n",
    "            continue\n",
    "        to_save = task_name if task_name[-1] != \"y\" else task_name[:-10]\n",
    "\n",
    "        # path to save dataset\n",
    "        save_path = \"../../../multimodal_dataset_SA_112/\" + user + \"_\" + to_save\n",
    "\n",
    "        # LOADING THE DATA\n",
    "        file = f\"./{user}/{task_name}.json\"\n",
    "        c = df[\"name\"] == file\n",
    "        filtered_df = df[c]\n",
    "        if len(filtered_df) == 0:\n",
    "            print(f\"User {user} on task {task_name} is not available in the csv file\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Creating dataset for user {user}, task {task_name}\") \n",
    "            i += 1\n",
    "\n",
    "            eeg_data = eeg_normalized[i]\n",
    "            eye_data = eye_normalized[i]\n",
    "            face_data = face_normalized[i]\n",
    "            \n",
    "            #label\n",
    "            l = filtered_df['labels'].iloc[0]\n",
    "\n",
    "            eeg_chunks = split_given_size(a=eeg_data, size=1)\n",
    "            eye_chunks = split_given_size(a=eye_data, size=1)\n",
    "            face_chunks = split_given_size(a=face_data, size=1)\n",
    "\n",
    "            for chunk in range(chunk_lengths[f\"{user}/{to_save}\"]):\n",
    "                video_path = \"../../../video_dataset_SA_112_30frames/\" + user + \"_\" + to_save\n",
    "                video_data = np.load(f\"{video_path}_chunk_{chunk}.npz\")\n",
    "                video_chunk = video_data[\"video\"]\n",
    "                label = video_data[\"label\"]\n",
    "                eeg_chunk = eeg_chunks[chunk].squeeze()\n",
    "                eye_chunk = eye_chunks[chunk].squeeze()\n",
    "                face_chunk = face_chunks[chunk].squeeze()\n",
    "                np.savez(save_path + \"_chunk_\" + str(chunk), video=video_chunk, eeg=eeg_chunk, eye=eye_chunk, face=face_chunk, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COGNITIVE LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./mental_demands_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_psd = []\n",
    "meta_pupil_diam = []\n",
    "meta_gaze_speed = []\n",
    "meta_fixation = []\n",
    "meta_scores = []\n",
    "meta_eye_landmarks = []\n",
    "meta_face_landmarks = []\n",
    "chunk_lengths = {}\n",
    "for user in sorted(os.listdir(\"../../../user_data/\")):\n",
    "    for task_name in sorted(os.listdir(\"../../../user_data/\" + user + \"/Task 1/\")):\n",
    "        if task_name == \".DS_Store\" or task_name == \"EEG\" or task_name == \"Eye Tracker\" or task_name == \"NASA\" or task_name == \"Video\":\n",
    "            continue\n",
    "        to_save = task_name if task_name[-1] != \"y\" else task_name[:-10]\n",
    "\n",
    "        # path to save dataset\n",
    "        save_path = \"../../../multimodal_dataset_CL_112/\" + user + \"_\" + to_save\n",
    "\n",
    "        # LOADING THE DATA\n",
    "        c1 = df[\"User ID\"] == int(user)\n",
    "        c2 = df[\"Task ID\"] == task_name\n",
    "        filtered_df = df[c1 & c2]\n",
    "        if len(filtered_df) == 0:\n",
    "            print(f\"User {user} on task {task_name} is not available in the csv file\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Parsing data files of user {user}, task {task_name}\")   \n",
    "\n",
    "            # PSD\n",
    "            psd = []\n",
    "            if task_name[-1] != 'y':\n",
    "                psd = loadmat(f\"../../../subject_data_for_chuhao/{user}/EEG/min_processed/backup/Task 1 Without Secondary Task/{to_save}.mat\")\n",
    "            else:\n",
    "                psd = loadmat(f\"../../../subject_data_for_chuhao/{user}/EEG/min_processed/backup/Task 1 With Secondary Task/{to_save}.mat\")       \n",
    "            meta_psd.append(psd['all_bands'][:,0:31,0:40])\n",
    "\n",
    "            # eye tracker\n",
    "            meta_fixation.append(np.loadtxt(f\"../../../subject_data_for_chuhao/{user}/Eye Tracker/Task 1/fixation/{task_name}.txt\"))\n",
    "            meta_pupil_diam.append(np.loadtxt(f\"../../../subject_data_for_chuhao/{user}/Eye Tracker/Task 1/pupil_diam/{task_name}.txt\"))\n",
    "            meta_gaze_speed.append(np.loadtxt(f\"../../../subject_data_for_chuhao/{user}/Eye Tracker/Task 1/gaze_speed/{task_name}.txt\"))\n",
    "\n",
    "            # facial landmarks\n",
    "            meta_eye_landmarks.append(np.loadtxt(f\"../../../subject_data_for_chuhao/{user}/Video/Task 1/Eye Landmarks/{task_name}.txt\",delimiter=','))\n",
    "            meta_face_landmarks.append(np.loadtxt(f\"../../../subject_data_for_chuhao/{user}/Video/Task 1/Face Landmarks/{task_name}.txt\",delimiter=','))\n",
    "            \n",
    "            #label\n",
    "            label = filtered_df['labels'].iloc[0]\n",
    "\n",
    "            # video\n",
    "            task_video_data = []\n",
    "            for frame in sorted(os.listdir(\"../../../user_data/\" + user + \"/Task 1/\" + task_name + \"/Images/\")):\n",
    "                image_path = \"../../../user_data/\" + user + \"/Task 1/\" + task_name + \"/Images/\" + frame\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                # print(image_path)\n",
    "                # image = custom_pad_image(image, target_size=(224, 224))\n",
    "                numpy_data = np.asarray(image).astype(np.float32)\n",
    "                numpy_data = numpy_data.transpose(2, 1, 0)\n",
    "                numpy_data = numpy_data / 255\n",
    "                task_video_data.append(numpy_data)\n",
    "            task_video_data = np.array(task_video_data)\n",
    "\n",
    "            chunks = split_given_size(a=task_video_data, size=30)\n",
    "            chunk_lengths[f\"{user}/{to_save}\"] = len(chunks)\n",
    "\n",
    "# PREPROCESSING AND NORMALIZATION     \n",
    "eeg_normalized = normalize_data(meta_psd)       \n",
    "eye_normalized = np.concatenate((normalize_data(meta_pupil_diam), normalize_data(meta_gaze_speed)), axis=3)\n",
    "face_normalized = np.concatenate((normalize_data(meta_eye_landmarks), normalize_data(meta_face_landmarks)), axis=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eeg_normalized.shape)\n",
    "print(eye_normalized.shape)\n",
    "print(face_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "for user in sorted(os.listdir(\"../../../user_data/\")):\n",
    "    for task_name in sorted(os.listdir(\"../../../user_data/\" + user + \"/Task 1/\")):\n",
    "        if task_name == \".DS_Store\" or task_name == \"EEG\" or task_name == \"Eye Tracker\" or task_name == \"NASA\" or task_name == \"Video\":\n",
    "            continue\n",
    "        to_save = task_name if task_name[-1] != \"y\" else task_name[:-10]\n",
    "\n",
    "        # path to save dataset\n",
    "        save_path = \"../../../multimodal_dataset_CL_112/\" + user + \"_\" + to_save\n",
    "\n",
    "        # LOADING THE DATA\n",
    "        file = f\"./{user}/{task_name}.json\"\n",
    "        c = df[\"name\"] == file\n",
    "        filtered_df = df[c]\n",
    "        if len(filtered_df) == 0:\n",
    "            print(f\"User {user} on task {task_name} is not available in the csv file\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Creating data file for user {user}, task {task_name}\") \n",
    "            i += 1\n",
    "\n",
    "            eeg_data = eeg_normalized[i]\n",
    "            eye_data = eye_normalized[i]\n",
    "            face_data = face_normalized[i]\n",
    "            \n",
    "            #label\n",
    "            l = filtered_df['labels'].iloc[0]\n",
    "\n",
    "            eeg_chunks = split_given_size(a=eeg_data, size=1)\n",
    "            eye_chunks = split_given_size(a=eye_data, size=1)\n",
    "            face_chunks = split_given_size(a=face_data, size=1)\n",
    "\n",
    "            for chunk in range(chunk_lengths[f\"{user}/{to_save}\"]):\n",
    "                video_path = \"../../../video_dataset_SA_112_30frames/\" + user + \"_\" + to_save\n",
    "                video_data = np.load(f\"{video_path}_chunk_{chunk}.npz\")\n",
    "                video_chunk = video_data[\"video\"]\n",
    "                label = video_data[\"label\"]\n",
    "                eeg_chunk = eeg_chunks[chunk].squeeze()\n",
    "                eye_chunk = eye_chunks[chunk].squeeze()\n",
    "                face_chunk = face_chunks[chunk].squeeze()\n",
    "                np.savez(save_path + \"_chunk_\" + str(chunk), video=video_chunk, eeg=eeg_chunk, eye=eye_chunk, face=face_chunk, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "js_video_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
